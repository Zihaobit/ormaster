<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>NLP with Neural Network</title>
    <meta name="viewport" content="width=device-width">
    <meta name="baidu-site-verification" content="r1z1K1EYR2" />
    <meta name="description" content="Welcome to Q.Liu's Homepage. Here you can find something about my research interests.">
    <link rel="canonical" href="http://localhost:4000/liuqianchao/update/2016/06/06/nlp/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <!-- 
    <link rel="stylesheet" href="/css/pygments.css"> -->

    <!-- mathjax -->
    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>


    <!-- GoogleAnalytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-69826873-1', 'auto');
      ga('send', 'pageview');

    </script>
    <!-- baidu spider-->
    <script>
    (function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
    })();
    </script>
</head>


    <body>

    <header class="site-header">
  <div class="wrap">

    <a class="site-title" href="/">Q.Liu</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>NLP with Neural Network</h1>
    <p class="meta">Jun 6, 2016</p>
  </header>

  <article class="post-content">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<p>    本文是使用神经网络的方法进行自然语言处理的技术路线，以Stanford的CS224d课程为主，仅供参考。</p>

<h3 id="1一些必要的知识">1.一些必要的知识</h3>
<p>    1.<strong>线性代数</strong>，包括矩阵的运算，以及模，秩，特征值、特征向量以及和导数求解相关的矩阵知识(比如牛顿法中的Hessian Matrix)等。 <br />
    2.<strong>概率论</strong>，包括概率密度函数，条件概率，联合概率，贝叶斯，协方差等。 <br />
    3.<strong>凸优化</strong>，包括什么是凸集合，凸函数，如何进行。</p>

<h3 id="2课程正文">2.课程正文</h3>

<h3 id="21词向量">2.1词向量</h3>

<p>    本小节的重点是word vectors，介绍了词汇向量化的几种经典的方法。</p>

<p>    首先介绍了使用descrete representation(比如one-hot)的方法进行词汇向量化的方法，其中一个典型的代表就是使用WordNet这样的<strong>字典</strong>,但是直接使用字典的方式来将词汇向量化，有很多问题存在：</p>
<ol>
  <li>会丢失很多词汇的本意细节(nuance), 比如一词多义的问题没办法保留多个意思。</li>
  <li>wordnet中的词汇有限，不能及时覆盖所有的词汇。</li>
  <li>不能直接用来计算词汇之间的相似性。</li>
</ol>

<p>    第二种表示方式是使用neighbors的count来表示词汇，其思想是<strong>一个词可以用句子中它周围的词来表示</strong>。该种方法有两种的选择，一个是使用“full document”，另一个种是使用“windows”；前者的思想又引出了latent semantic analysis(可以用于文章分类等)；使用neighbors来进行词汇的表达存在的问题包括：</p>
<ol>
  <li>需要维护高维矩阵，该矩阵需要大量的存储空间</li>
  <li>模型的鲁棒性不高   <br />
    由此，针对上述问题，提出的解决方式是进行矩阵降维，常用的方法包括：SVD（对于<script type="math/tex">m n</script>矩阵，时间复杂度为<script type="math/tex">o(mn^{2})</script>，其中<script type="math/tex">% <![CDATA[
n< m %]]></script>）,此外SVD分解的方法实现词汇向量化对于新词汇的加入不是很友好。</li>
</ol>

<p>    除了第二种方法中的使用临近词汇的count来表示，并通过一定的降维方式来实现word embedding外，还有一种方式是<strong>直接生成低维向量</strong>(其实这个低维一般也有几百维)。该种方法的思想是通过训练机器学习模型来“modeling”词向量,其代表方法包括Google的word2vec，其特点是对于新的词汇有较好的接收。该种方法通过建立目标函数，使用梯度下降(链式法则)的方法来maximize/minimize目标函数。 <br />
    其基本步骤包括：</p>
<ol>
  <li>定义目标函数/Cost</li>
  <li>small random numbers初始化参数（对于Word2vec 参数为2<em>V</em>d个数，其中V为字典长度，d为每个word的词向量维度,这里的2是因为对于每一个词有v,u两个向量来表示,一般使用词向量时，v+u作为结果）</li>
  <li>对Cost进行SGD(Updating parameters with each window in Word2Vec) <br />
    针对Word2vec这种方法可以展开思考一些问题，比如每个window的词比较少，这样梯度是稀疏的(参数中只有2c+1个参数需要更新，w为window长)，为解决该问题，只处理(更新)目标2c+1个参数(by only update certain columns of full embedding matrix U(left or right words) and V(center word))。</li>
</ol>

<p>    下面给出Word2Vec的目标函数的子项(每一个窗口内的neighbor词<script type="math/tex">o</script>均对应下面给的exponent概率)：</p>

<div align="center">$$p(o|c)=\frac{exp(v_{c}^{T}u_{o})}{\sum_{w=1}^W exp(v_{c}^{T} u_{w})}$$  </div>

<p>    In above formula, we could find that we need to sum all the exponent function of neighbor words, which is computationally expensive. Hence, we change the objective function.</p>

<div align="center">$$J(\theta)=log \sigma(u_{o}^{T}v_{c}) + \sum_{t=1}^k E_{j \~ P(w)}[log \sigma(-u_{j}^{T}v_{c})]$$  </div>

<p>    改成上式后的方法，叫做Negative Sampling, 其中<script type="math/tex">k</script>可以取10，20这样的数；<script type="math/tex">\sigma</script>是sigmoid function。<script type="math/tex">\sigma(-x)=1-\sigma(x)</script>；Rondom函数有特定的设计，corpus中频繁出现的词，更有可能被选出来，但是这个可能性要低于它在corpus中出现的频率。我们认为Negative Sampling相比于之前的目标函数，增加了额外的信息：使不相关的(random)的词汇距离尽可能远。</p>

<h3 id="22-word-window-classification-and-nerual-network">2.2 Word window classification and nerual network</h3>
<p>    分类，在自然语言中的用处包括：从词到词的预测(比如说补全句子)，情感分析，命名实体识别（named entity），分析决策（sell/buy decision），翻译等。下面将给出分类的相关内容。  <br />
    <strong>LR</strong>,Logistic Regression(之前blog里介绍过的SVM只会给出分类的结果，而LG会给出分类的概率，比如分为正类的可能性)，LG是使用sigmoid函数，相当于线性回归之后加入了Logistic变换。</p>

<p>    NLP的Classification与传统意义上的classification的不同在于，NLP中除了训练模型的参数外，还可以把词向量加入到需要训练的参数中，并且，当训练集非常大时，同时进行词向量的训练(update)，效果较好。 <br />
然后引入Window classificaition,一般对单个word的分类，在实际场景中很少用到，此外，word在句子中包含的信息会更多，因此，我们对NLP进行分类时，一般是指window classification。</p>

<p>    单层神经网络(one hidden layer nn/three layers nn)，对于windows classification问题，比如其中一个sentence窗口是”Museums in Pairs are amazing”，用<script type="math/tex">x</script>来表示,且<script type="math/tex">x \in \mathbb{R}^{20 \times 1}</script>，然后根据我们设计的隐层metric（结点的数目），可以假设<script type="math/tex">W \in \mathbb{R}^{8 \times 20}</script>,由隐层到输出层的映射<script type="math/tex">U \in \mathbb{R}^{8 \times 1}</script>,可得<script type="math/tex">s = U^{T} \times f(Wx+b) \in \mathbb{R}</script>,s即为分类的得分(score)。 <br />
    如果说”Museums in Pairs are amazing”是True window,那么可以定义一些corrupt windows,比如(not all museums in Paris),通过训练，使true windows的得分<script type="math/tex">S</script>高，corrupt windows得分<script type="math/tex">S_{c}</script>低。比如可以将目标函数定义为：</p>

<div align="center">$$minimize J,  J = max(0,1-S+S_{c})$$  </div>

<p>    该目标函数叫做Max-Margin Objective function,在实际操作中一般忽略左侧的max和0。对于每一个true window，都需要sample几个corrupt windows。 <br />
    这里尝试推导<script type="math/tex">W_{ij}</script>的梯度：</p>

<div align="center">$$\frac{\Delta S}{\Delta W_{ij}} = U_{i} \frac{\Delta f(Wx+b)}{\Delta W_{ij}} = U_{i} \frac{f^{'}(Wx+b) \Delta(Wx+b)}{\Delta W_{ij}} = U_{i} f^{'}(Wx+b) x_{j} = U_{i}f^{'}(z_{i})x_{j}= \delta _{i}x_{j}$$    </div>

<p>    当<script type="math/tex">f(z)</script>为sigmoid function时，<script type="math/tex">f^{'}(z) = 1 - f(z)</script>,且有<script type="math/tex">z_{i} = w_{i}x+b_{i} = \sum_{j=1}^n W_{ij}x_{j} + b_{i}</script>,其中<script type="math/tex">\delta _{i}</script>称为local error signal，<script type="math/tex">x_{j}</script>称为local input signal</p>

<p>    接下来介绍第一个tip，反向传播计算时的参数复用，<script type="math/tex">\frac{\Delta S}{\Delta W_{ij}} = U_{i}f^{'}(z_{i})x_{j}=U_{i}(1-f(z_{i}))x_{j}</script>,作为activation <script type="math/tex">a</script>的error message (responsibility). 这样就可以省去计算了；同理可以求得biases <script type="math/tex">b</script>，可知<script type="math/tex">b</script>是完全不增加新的计算量的。</p>

<p>    第二个tip，Multi-task learning/Weight sharing。其思想是可以通过参数共享来同时训练多个tasks，比如NLP中的词性标注POS(Part-of-Speech) ，NER（Named Entity Recognition）可以同时进行，两个模型在隐层使用同样的weights，但在output层使用不同的weights参数。</p>

<p>    接下来介绍了一些训练神经网络的细节：</p>

<p>    1. 激活函数：1）<script type="math/tex">logistic</script>(sigmoid)，<script type="math/tex">f^{'}(z)=f(z)(1-f(z))</script>； <br />
                        2）<script type="math/tex">tanh</script>:<script type="math/tex">\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}</script>,并且<script type="math/tex">tanh(z)=2logistic(2z)-1</script>，<script type="math/tex">f^{'}(z)=1-f(z)^{2}</script> <br />
                        3) <script type="math/tex">ReLu</script>，<script type="math/tex">max(0,x)</script></p>

<p>    2. Gradient check，作为神经网络中的“debug”，gradient check可以帮助我们确定梯度计算的准确性。梯度的计算是通过pen-and-paper（推导出来的导数式），而check的时候，通过数值近似的方法来计算：<script type="math/tex">f^{'}(\theta)=\frac{J(\theta + \epsilon)+J(\theta - \epsilon)}{2\epsilon}</script>，其中<script type="math/tex">\epsilon</script>非常小，为<script type="math/tex">10^{-4}</script>数量级，然后比较两个计算结果，确定其相差不多。</p>

<p>    3. Model simplification, start from simplest model to find bugs.</p>

<p>    4. Parameter Initialization, 假设<script type="math/tex">W_{ij} \in [-1,1]</script>，<script type="math/tex">x_{j} \in [-100,100]</script>，并且<script type="math/tex">b_{i} \in [-100,100]</script>，这样，对于<script type="math/tex">a_{i}=f(z_{i})=tanh(z_{i})</script>，<script type="math/tex">z_{i}</script>过大，对导致tanh的梯度接近于0，不利于优化。通常情况下我们将<script type="math/tex">b_{i}</script>初始化为0，<script type="math/tex">W_{ij}\~uniform(-r,r)</script>
，并且<script type="math/tex">r=\sqrt[]{6/(fan.in + fan.out)}</script>，其中<script type="math/tex">fan.in</script>是上一层layer，units的个数。注意上式是tanh的参考值，对于sigmoid，应该乘上4.</p>

<p>    5. SGD, 更新参数的频率,对于大的数据集，SGD优于all batch methods。对于小的数据集,L-BFGS 或者Conjugate Gradients方法更合适。比较常用的是mini-batch SGD, 20到1000大小的batch，这样也有利于并行计算。对于参数的更新，除了最基础的learning-rate <script type="math/tex">\times</script> gradient,还有一些延伸： <br />
                        1)Momentum,由于SGD的更新完全依赖于当前的batch，这种更新方向可能是十分不稳定的，于是在此处引入momentum，在该次batch的更新中，保留上次更新的影响。<script type="math/tex">v=uv-\alpha \delta_{\theta}J_{t}(\theta)</script>，<script type="math/tex">v</script>的初始值是0，<script type="math/tex">u</script>为0到1的正数，可取0.9。Momentum的收敛速度更快，而且能够跳出局部最优点。除此之外，还可以改变learning rate <script type="math/tex">\alpha</script>：当training validation error不再改变时，<script type="math/tex">\alpha</script>减0.5；此外对于频繁出现的word，可以使用较低的learning rate，对于rare的word，使用较高的learning rate。</p>

<p>    6. Regularize,当遇到样本小，参数多的情况，可能会有过拟合的现象发生，这里引入解决办法：1，最简单的方法是改变Model Size，减少结点的数目，层的数目。2，Standard L1 or L2 regularization on weights。3，early stop,use parameters that gave best validation error。4，Drop out，1）在training time,随机将50units结点的输入设为0；2)在testing time,将所有的weight除以2(修补1带来的部分影响)</p>

<h3 id="23-language-model">2.3 Language model</h3>
<p>    A <strong>language model</strong> computes a probability for a sequence of words：<script type="math/tex">P(w_{1},w_{2}...w_{T})</script>。有了language model,便可以知道哪些word拼凑起来更像一个句子，比如the cat is small比small the cat is更符合我们对于句子的定义，这些特性在machine translation中帮助我们进行翻译。我们知道对于条件概率：</p>

<div align="center">$$P(w_{1},w_{2}...w_{m}) = P(w_{1})*P(w_{2}|w_{1})*P(w_{3}|w_{1}w_{2})..P(w_{m}|w_{1}w_{2}..w_{m-1}) = \prod_{i=1}^m P(w_{i}|w_{1}..w_{i-1}) \approx \prod_{i=1}^m P(w_{i}|w_{i-(n-1)}..w_{i-1})$$  </div>

<p>    不同于之前讲过的windows classification，在language model中fixed window 可能不是很合理，前5个/10词汇未必一定能很好地引出下一个词汇，因此，我们在这里引入RNN(Recurrent NN)</p>

<p>    输入是word vectors：<script type="math/tex">x_{1}...x_{t-1},x_{t},x_{t+1}...x_{T}</script>,其中<script type="math/tex">T</script>是whole corpus的大小。对于hidden层的一个unit,<script type="math/tex">h_{t} = \sigma(w^{hh}h_{t-1} + w^{hx}x_{t})</script>，输出为<script type="math/tex">\hat{y}_{t}=softmax(w^{s}h_{t})</script>，其中<script type="math/tex">w^{hh} \in \mathbb{R}^{D_{h} \times D_{h}}</script>，其中<script type="math/tex">D_{h}</script>为hidden layer的数据维度。<script type="math/tex">w^{hx} \in \mathbb{R}^{D_{h} \times d}</script>，<script type="math/tex">d</script>为word vector的维度，<script type="math/tex">w^{s} \in \mathbb{R}^{V \times D_{h}}</script>。因此得到<script type="math/tex">\hat{y} \in \mathbb{R}^{V}</script>,是word vocabulary的概率分布。每个predicting word都对应完成的word vocabulary distribution。因此我们定义目标函数为cross entropy loss:</p>

<div align="center"> $$J(\theta) = -\frac{1}{T} \sum_{t=1}^T \sum_{j=1}^V y_{t,j} \ log_{2} \ \hat{y}_{t,j}$$ </div>

<p>    最后<script type="math/tex">minimal w 2^{J}</script></p>

<p>    这里需要指出rnn的问题，我们知道由于input层(hidden层的time step)比较长(这个长度由corpus的sentence长度决定，可以是5也可以是50,也可以是一本书几万个word)，这就会导致gradient vanishing 和gradient explosion。下面推导gradient vanishing的原因。</p>

<div align="center">
<img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/rnn-bptt1.png" height="200" weight="500" /> </div>

<div align="center"> 
$$\frac{\Delta E_{3}}{\Delta w} =\sum_{i=1}^{3} \frac{\Delta E_{3}}{\Delta \hat{y}_{3}} \frac{\Delta \hat{y}_{3}}{\Delta s_{3}}  \frac{\Delta s_{3}}{\Delta s_{i}}  \frac{\Delta s_{i}}{\Delta w}$$
</div>
<p>    之前提到的简单Feedforward NN中没有weights sharing，在RNN中由于time step间的<script type="math/tex">w_{hh}</script>是一样的，所以在上式中，我们对所有time step关于<script type="math/tex">w_{hh}</script>的梯度取和。其中<script type="math/tex">\frac{\Delta s_{3}}{\Delta s_{i}} = \prod_{j=i+1}^{3} \frac{\Delta s_{j}}{\Delta s_{j-1}}</script>。我们又知道<script type="math/tex">s_{j}=f(w s_{j-1}+b)</script>，故<script type="math/tex">\frac{\Delta s_{j}}{\Delta s_{j-1}} = w f^{'}(w s_{j-1}+b)</script>，易证<script type="math/tex">\frac{\Delta s_{3}}{\Delta s_{i}}</script>将会是非常大，或是非常小的数(Bengio et al 1994)。</p>

<p>    对于Gradient explosion问题，Mikolov提出了用clip gradients的方法解决问题，是通过设置阈值(threshold)，如果梯度大于这个阈值，便手动修改梯度。对于gradient vanishing 问题我们的非线性函数采用relu，将w初始化为单位矩阵（这种RNN称为IRNN），可以一定程度上解决问题。</p>

<p>    在RNN的基础上，提出了一部分的<strong>改进模型</strong>，比如bidirectional RNNs,其time step不仅包含<script type="math/tex">h_{t-1}</script>的信息，还包含了<script type="math/tex">h_{t+1}</script>。</p>

<p>    接下来在Lecture9中谈及了Machine Translation的基本模型（encode以及decode两部分RNN组成），以及其五种扩展(model extensions)；随后介绍了改进模型Gates Recurrent Units，其改变<script type="math/tex">W_{hh}h_{t-1}+W_{hx}x_{t}</script>为Gate，包括Update gate：<script type="math/tex">z_{t} = \sigma(W^{z}x_{t}+U^{z}h_{t-1})</script>，以及reset gate：<script type="math/tex">r_{t} = \sigma(W^{r}x_{t}+U^{r}h_{t-1})</script>，设置中间值<script type="math/tex">\hat{h}</script>，有<script type="math/tex">\hat{h}_{t}=tanh(Wx_{t} + r_{t} \circ Uh_{t-1})</script>，<script type="math/tex">h_{t}=z_{t} \circ h_{t-1} + (1-z_{t}) \circ \hat{h}_{t}</script></p>

<p>    <strong>Deep learning public library:TensorFlow</strong></p>

<p>    分类：1)通过configuration file文件完成模型：Caffe，DistBelief，CNTK；2）programmatic generation：Torch（Lua），Theano（Python），Tensorflow（Python）。如我们之前Blog提到的，Theano和Tensorflow是非常相似的，这里选择介绍Tensorflow，其一方面有google的背书，另一方面对分布式计算支持较好（一般模型需要在16核GPU上训练1周之久）。</p>

<p>    1. Tensors，用于表示数据，可以看做是n维数组，可以理解为numpy最基础的功能，而TF则在numpy的基础上提供了tensor funciton, 梯度计算，GPU support这些额外的功能。首先介绍了numpy和tensorflow的相似之处，包括metrics的定义，求和以及broadcast性质。然后指出了两者的不同，在numpy中定义metric时，已经在内存中malloc了相应的参数，而在tf中，仅仅是定义了computation graph，直至evaluated(.eval())时才真正create numerical value。</p>

<p>    2. Session, 是所有operation的环境，sess.run(y)返回fetch（从global computation graph 从返回的结果），一般将要参与运算的所有tensor定义在同一个session下。tf.InteractiveSession()是在ipython中使用的default session.</p>

<p>    3. Computation Graph, every function(<script type="math/tex">\times \ and \ other \ operation</script>),就是在computation graph上加入新内容。Computation Graph会record这些操作。</p>

<p>    4. Variables,是boxs that hold states. 与constant tensor的区别是，variable在使用(session.run(v)前必须initialize(tf.initialize_all_variables())。variable为什么需要name：通过name来在computation graph 中进行索引。variable的scope(namespace)问题：，背景：复杂的神经网络模型中可能会有数百个variable，我们需要了解namespace。对于Recurrent NN中的weights sharing，我们使用reuse_varaiables()</p>

<p>    5. Inputting data，定义数据。1）方法一是通过numpy定义metric a，使用tf.convert_to_tensor(a)完成；2）tf.placeholder，dummy nodes （相比于numpy的方法，更符合computation graph的定义）that provide entry points for data to computational graph.通过feed_dict完成从placeholder到numerial data 到映射。</p>

<h4 id="reference">Reference</h4>
<ol>
  <li>Stanford <a href="http://cs224d.stanford.edu/index.html">cs224d</a></li>
</ol>


  </article>


</div>

<br>
<br>
<br>
<br>
<br>
<div id="disqus_thread"></div>

<script>
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//liuqianchao.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

      </div>
    </div>

    
<footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">Q.Liu</h2>

    <div class="footer-col-1 column">
      <ul>
        <li>Q.Liu</li>
        <li><a href="mailto:liuqianchao@163.com">liuqianchao@163.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/qianchaoliu">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">qianchaoliu</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/liuqianchao">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">liuqianchao</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Welcome to Q.Liu's Homepage. Here you can find something about my research interests.</p>
    </div>

  </div>

</footer>




    </body>
</html>