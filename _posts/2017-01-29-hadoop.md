---
layout: post
title:  "Hadoop"
date:   2017-01-29 00:00:04
categories: liuqianchao update
---


### 1. Installing Hadoop

&emsp; 本小节会介绍在MacOS Sierra(10.12)下安装hadoop、hive以及web管理平台hue.   
&emsp; &emsp; **步骤一、下载hadoop**    
&emsp; &emsp; 下载binary文件(.tar.gz) ，使用"tar -xzf"解压。   
&emsp; &emsp; 解压后的文件，使用下面的bash语句，添加到环境变量中（其中XXXX代表解压后的hadoop文件的主目录），添加环境变量的步骤是可选的。   

{% highlight python %}
echo "export HADOOP_HOME=XXXX" >> $HOME/.bashrc
echo "PATH=$PATH:$HADOOP_HOME/bin" >> $HOME/.bashrc
source $HOME/.bashrc
{% endhighlight %}

&emsp; &emsp; **步骤二、配置hadoop**    
&emsp; &emsp; 1)首先修改"./etc/hadoop/core-site.xml":

{% highlight python %}
<configuration>
  <property>
     <name>hadoop.tmp.dir</name>  
     <value>/usrs/liuqianchao/hadoop/hdfs/tmp</value>
  </property>
  <property>
     <name>fs.default.name</name>                                     
     <value>hdfs://localhost:9000</value>                             
  </property>
<property>  
</configuration>
{% endhighlight %}

&emsp; &emsp; 其中**hadoop.tmp.dir**为hadoop文件系统的基本路径，如果在hdfs-sited.xml中不指定namenode和datanode的存放地址，其数据就存放在hadoop.tmp.dir中。**hadoop.tmp.dir**定义了NameNode的URI。   
&emsp; &emsp; 2)接下来修改"./etc/hadoop/hdfs-site.xml"：

{% highlight python %}
<configuration>
 <property>
     <name>dfs.replication</name>
     <value>1</value>
 </property>
</configuration>
{% endhighlight %}

&emsp; &emsp; 其中**dfs.replication**是指数据块在整个hadoop系统中备份的个数，默认情况下是3份，这里由于是单机情况下，设置成1份。此外在该文件下还可以指定**dfs.name.dir、dfs.data.dir**来定义namenode与datanode在本地文件系统中的存储路径，这里没有进行指定；数据会被存放在**hadoop.tmp.dir**指定的路径下。   
&emsp; &emsp; 3)修改"./etc/hadoop/mapred-site.xml"：

{% highlight python %}
<configuration>
   <property>
     <name>mapred.job.tracker</name>
     <value>localhost:9010</value>
   </property>
 </configuration>
{% endhighlight %}
&emsp; &emsp; 其中**mapred.job.tracker**定义了map-reduce执行的主机和端口。

&emsp; &emsp; **步骤三、启动hadoop**    
&emsp; &emsp; 在其中之前，需要进入hadoop根目录下的bin文件夹，执行"./hadoop namenode –format"来格式化工作空间；    
&emsp; &emsp; 在完成上述操作后，便可进入hadoop根目录下的sbin文件夹，执行"./start-dfs.sh"来启动hdfs. 并通过下列本地地址来验证是否成功启动。

&emsp; &emsp; HDFS status: <a>http://localhost:50070</a>   
&emsp; &emsp; Cluster Status: <a>http://localhost:8088</a>   
&emsp; &emsp; Specific Node Information: <a>http://localhost:8042</a>   
&emsp; &emsp; SecondaryNamenode <a>http://localhost:50090</a>   