---
layout: post
title:  "Bayes Classification"
date:   2016-04-27 10:00:04
categories: liuqianchao update
---

&nbsp;&nbsp;&nbsp;&nbsp;和SVM、神经网络这些分类器一样，贝叶斯分类的基本思想也是通过设定一个“损失函数”，通过最小化损失函数来实现分类结果的最优。


### 1.贝叶斯分类中的损失函数是怎么定义的？
&nbsp;&nbsp;&nbsp;&nbsp;首先，定义损失$$\lambda_{ij}$$是将本属于$$c_{j}$$的样本$$x$$错误地分为$$c_{i}$$带来的损失，且$$\lambda_{jj}=0$$,因此对于N类分类问题，一个样本的期望损失为：

<div align="center">$$R(c_{i} \mid x) = \sum_{i=1}^N \lambda_{ij}P(c_{i} \mid x)$$</div>

&nbsp;&nbsp;&nbsp;&nbsp;假设$$i≠j$$，令$$\lambda_{ij}=1$$, 如果每个样本$$x$$的期望损失都能最小化，总体损失必然最小化，

<div align="center">$$arg min R(c_{i} \mid x)$$</div>

&nbsp;&nbsp;&nbsp;&nbsp;依照惯例我们进行转换，得出$$R(c_{i} \mid x) = 1 - P(c_{i} \mid x)$$,即：

<div align="center">$$arg max P(c_{i} \mid x)$$</div>


### 2.朴素贝叶斯
&nbsp;&nbsp;&nbsp;&nbsp;在我们列出来目标函数后，该谈一谈贝叶斯分类器是如何进行计算的了。

&nbsp;&nbsp;&nbsp;&nbsp;首先，给出条件概率、贝叶斯公式的定义：

&nbsp;&nbsp;&nbsp;&nbsp;条件概率$$P(A \mid B)$$的含义是在B事件发生的基础上发生A事件的概率,即：   

<div align="center">$$P(A \mid B) = \frac{P(AB)}{P(B)}$$</div>

&nbsp;&nbsp;&nbsp;&nbsp;而贝叶斯公式则指出了如何借助$$P(A \mid B)$$计算出$$P(AB)$$，继而得出$$P(B \mid A)$$：

<div align="center">$$P(B \mid A) = \frac{P(A \mid B)P(B)}{P(A)}$$</div>

&nbsp;&nbsp;&nbsp;&nbsp;我们在一个N类分类问题中，一个样本$$x$$,被划分到各类的概率分别为$$P(y_{1} \mid x)$$, $$P(y_{2} \mid x)$$...$$P(y_{N} \mid x)$$。朴素贝叶斯的分类方法即取条件概率最大的$$y_{i}$$作为分类结果。

&nbsp;&nbsp;&nbsp;&nbsp;因此我们在进行分类时，将问题转为了如何计算条件概率$$P(y_{i} \mid x)$$， 由贝叶斯公式我们知道:

<div align="center">$$P(y_{i} \mid x) = \frac{P(x \mid y_{i})P(y_{i})}{P(x)}$$</div>

&nbsp;&nbsp;&nbsp;&nbsp;其中$$P(y_{i})$$是“先验概率”，可以由大量的样本数据(训练集)直接给出；而$$P(x)$$对于所有的样本个体均相同，作为一个分母，我们可以直接当做一个“缩放因子”不予考虑；    
&nbsp;&nbsp;&nbsp;&nbsp;剩下要计算的就是$$P(x \mid y_{i})$$，其由$$x$$的各个属性关于$$y_{i}$$的条件概率的联合概率决定，在假设$$x$$的各个属性$$a_{1}$$, $$a_{2}$$...$$a_{m}$$相互独立的情况下:

<div align="center">$$P(x \mid y_{i}) =\sum_{i=1}^m P(a_{i} \mid y_{i})$$</div>

&nbsp;&nbsp;&nbsp;&nbsp;上式可以直接通过训练集计算得出，以上便是朴素贝叶斯分类器的定义，但根据其定义不难得出朴素贝叶斯存在两处明显不足：   
&nbsp;&nbsp;&nbsp;&nbsp;**一方面**一旦属性可选的离散变量较多，样本空间会急遽增大（假设一共有10个属性，每个属性都是二值的，则有$$2^{10}$$个样本属性变化范围），需要大量的训练集才能进行有效的分类。一旦某分类下的某一属性值未在样本中出现，即$$P(a_{i} \mid y_{i})=0$$，由于我们在计算条件概率时是采用联合概率的乘积形式，于是计算出来的$$P(x \mid y_{i})=0$$, 个体属性值的缺少会对抹去其它属性的价值。   
&nbsp;&nbsp;&nbsp;&nbsp;**另一方面**其适用于属性间相互独立的情景。

### 3.贝叶斯分类的改进

&nbsp;&nbsp;&nbsp;&nbsp;**针对于第一不足**，我们可以引入Laplacian Correction，比如对于辨别某犬是否为阿拉斯加犬时，其中一个属性是眼球颜色，该选项共有3个可能，但在训练集中仅出现了其中两种，则对第三种眼球颜色，在计算其$$P(a_{i} \mid y_{i})$$时，假定种类$$y_{i}=10$$，则按照以往的计算方式有：   

<div align="center">$$P(a_{i} \mid y_{i})=\frac{0}{10}=0$$</div>

在经过修正后：

<div align="center">$$P(a_{i} \mid y_{i})=\frac{0+1}{10+3}=\frac{1}{13}$$</div>

其中分子中的1为固定值，分母中的3对应三种可选眼球颜色。   
&nbsp;&nbsp;&nbsp;&nbsp;初此之外，将通过对联合概率取对数将“连乘”转换为“连加”的方式也能解决未出现属性值带来的“致命影响”（也能一定程度上解决浮点数下溢问题）。       
&nbsp;&nbsp;&nbsp;&nbsp;以上方法虽然能够一定程度上解决第一处不足，但样本属性空间的增加对样本数据的大量需求仍存在。

&nbsp;&nbsp;&nbsp;&nbsp;**针对于第二处不足**，又提出了“半朴素贝叶斯”和“贝叶斯网络”。




#### Reference
1. Zhihua Zhou [Machine Learning](https://www.amazon.cn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E/dp/B01ARKEV1G/ref=sr_1_1?ie=UTF8&qid=1461769135&sr=8-1&keywords=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)

